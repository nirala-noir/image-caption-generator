{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image caption generator",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO19DQ2FucqjnX9fDpavCEs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nirala-noir/image-caption-generator/blob/main/image_caption_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vEHb_49ME63"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmwkOuZ_MKJc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKya3O0vMTll"
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex-JKUvKMfX-"
      },
      "source": [
        "image_dataset_path = '/content/drive/MyDrive/image caption generator/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "caption_dataset_path = '/content/drive/MyDrive/image caption generator/Flickr8k_text/Flickr8k.token.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sI4xXjjNeXh"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('/content/drive/MyDrive/image caption generator/Flickr8k_Dataset/Flicker8k_Dataset/1000268201_693b08cb0e.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yT88pYDNozc"
      },
      "source": [
        "# load the caption file & read it\n",
        "def load_caption_file(path):\n",
        "    \n",
        "    # dictionary to store captions\n",
        "    captions_dict = {}\n",
        "    \n",
        "    # iterate through the file\n",
        "    for caption in open(path):\n",
        "    \n",
        "        # caption has format-> 1000268201_693b08cb0e.jpg#0  A child in a pink dress is climbing up a set of stairs in an entry way .\n",
        "        tokens = caption.split()\n",
        "        caption_id, caption_text = tokens[0].split('.')[0], tokens[1:]\n",
        "        caption_text = ' '.join(caption_text)\n",
        "        \n",
        "        # save it in the captions dictionary\n",
        "        if caption_id not in captions_dict:\n",
        "            captions_dict[caption_id] = caption_text\n",
        "        \n",
        "    return captions_dict\n",
        "\n",
        "# call the function\n",
        "captions_dict = load_caption_file(caption_dataset_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1coTJDINu-e"
      },
      "source": [
        "# clean the captions\n",
        "import string\n",
        "\n",
        "# dictionary to store the cleaned captions\n",
        "new_captions_dict = {}\n",
        "\n",
        "# prepare translation table for removing punctuation. third argument is the list of punctuations we want to remove\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "# loop through the dictionary\n",
        "for caption_id, caption_text in captions_dict.items():\n",
        "    # tokenize the caption_text\n",
        "    caption_text = caption_text.split()\n",
        "    # convert it into lower case\n",
        "    caption_text = [token.lower() for token in caption_text]\n",
        "    # remove punctuation from each token\n",
        "    caption_text = [token.translate(table) for token in caption_text]\n",
        "    # remove all the single letter tokens like 'a', 's'\n",
        "    caption_text = [token for token in caption_text if len(token)>1]\n",
        "    # store the cleaned captions\n",
        "    new_captions_dict[caption_id] = 'startseq ' + ' '.join(caption_text) + ' endseq'\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtPm4mWpNyFx"
      },
      "source": [
        "# delete unwanted \n",
        "del captions_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-hNrgjZN0yg"
      },
      "source": [
        "print('\"' + list(new_captions_dict.keys())[0] + '\"' + ' : ' + new_captions_dict[list(new_captions_dict.keys())[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDPlkvrN3PI"
      },
      "source": [
        "len(new_captions_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkX10je2N58a"
      },
      "source": [
        "#Make a list of only those images who has caption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmP8VdthN9Fa"
      },
      "source": [
        "caption_images_list = []\n",
        "\n",
        "image_index = list(new_captions_dict.keys())\n",
        "\n",
        "caption_images_list = [ image.split('.')[0] for image in os.listdir(image_dataset_path) if image.split('.')[0] in image_index ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfgu08xVN_6t"
      },
      "source": [
        "caption_images_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRVz8zfmOCYZ"
      },
      "source": [
        "#Make training, validation and test data\n",
        "\n",
        "#taking 7081 images for training, 1000 for validation and rest 10 for testing¶"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maV3GXxVOH4Z"
      },
      "source": [
        "train_validate_images = caption_images_list[0:8081]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXEydIEsOJkR"
      },
      "source": [
        "test_images = caption_images_list[8081:8091]\n",
        "test_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvO7ECocOMhg"
      },
      "source": [
        "#Image Feature Extractor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ksLk3IAOPni"
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "def extract_features(directory, image_keys):\n",
        "    # load the model\n",
        "    model = VGG16()\n",
        "    \n",
        "    # re-structure the model\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    \n",
        "    # summarize\n",
        "    print(model.summary())\n",
        "    \n",
        "    # extract features from each photo\n",
        "    features = dict()\n",
        "    \n",
        "    for name in image_keys:\n",
        "        \n",
        "        # load an image from file\n",
        "        filename = directory + '/' + name + '.jpg'\n",
        "        \n",
        "        # load the image and convert it into target size of 224*224\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        \n",
        "        # convert the image pixels to a numpy array\n",
        "        image = img_to_array(image)\n",
        "        \n",
        "        # reshape data for the model\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        \n",
        "        # prepare the image for the VGG model\n",
        "        image = preprocess_input(image)\n",
        "        \n",
        "        # get features\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        \n",
        "        # get image id\n",
        "        image_id = name.split('.')[0]\n",
        "        \n",
        "        # store feature\n",
        "        features[image_id] = feature\n",
        "        \n",
        "#         print('>%s' % name)\n",
        "        \n",
        "\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsEaCpBeOTui"
      },
      "source": [
        "# extracting image features for train_validate_images\n",
        "train_validate_features = extract_features(image_dataset_path, train_validate_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDvzsKJiOzZf"
      },
      "source": [
        "print(\"{} : {}\".format(list(train_validate_features.keys())[0], train_validate_features[list(train_validate_features.keys())[0]] ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47qcvUieO3Lg"
      },
      "source": [
        "len(train_validate_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIudrl0RO6cW"
      },
      "source": [
        "from pickle import dump\n",
        "dump(train_validate_features, open('./train_validate_features.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q41MXCv3O8ng"
      },
      "source": [
        "#Preparing the input data¶"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFmKdQiDO-HX"
      },
      "source": [
        "# load libraries\n",
        "import numpy as np\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout, LSTM, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idwqsozJPAus"
      },
      "source": [
        "# make a dictionary of image with caption for train_validate_images\n",
        "train_validate_image_caption = {}\n",
        "\n",
        "for image, caption in new_captions_dict.items():\n",
        "    \n",
        "    # check whether the image is available in both train_validate_images list and train_validate_features dictionary\n",
        "    if image in train_validate_images and image in list(train_validate_features.keys()):\n",
        "        \n",
        "         train_validate_image_caption.update({image : caption})\n",
        "\n",
        "len(train_validate_image_caption)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNIFHJo5PDEI"
      },
      "source": [
        "#make sure the correct caption is mapped with the correct image\n",
        "\n",
        "list(train_validate_image_caption.values())[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HqbeycPFxA"
      },
      "source": [
        "Image(image_dataset_path+'/'+list(train_validate_image_caption.keys())[1]+'.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR3s3CwZPHhy"
      },
      "source": [
        "# initialise tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# create word count dictionary on the captions list\n",
        "tokenizer.fit_on_texts(list(train_validate_image_caption.values()))\n",
        "\n",
        "# how many words are there in the vocabulary? store the total length in vocab_len and add 1 because word_index starts with 1 not 0 \n",
        "vocab_len = len(tokenizer.word_index) + 1\n",
        "\n",
        "# store the length of the maximum sentence\n",
        "max_len = max(len(train_validate_image_caption[image].split()) for image in train_validate_image_caption)\n",
        "\n",
        "def prepare_data(image_keys):\n",
        "    \n",
        "    # x1 will store the image feature, x2 will store one sequence and y will store the next sequence\n",
        "    x1, x2, y = [], [], []\n",
        "\n",
        "    # iterate through all the images \n",
        "    for image in image_keys:\n",
        "\n",
        "        # store the caption of that image\n",
        "        caption = train_validate_image_caption[image]\n",
        "\n",
        "        # split the image into tokens\n",
        "        caption = caption.split()\n",
        "\n",
        "        # generate integer sequences of the\n",
        "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "        length = len(seq)\n",
        "\n",
        "        for i in range(1, length):\n",
        "\n",
        "            x2_seq, y_seq = seq[:i] , seq[i]  \n",
        "\n",
        "            # pad the sequences\n",
        "            x2_seq = pad_sequences([x2_seq], maxlen = max_len)[0]\n",
        "\n",
        "\n",
        "            # encode the output sequence                \n",
        "            y_seq = to_categorical([y_seq], num_classes = vocab_len)[0]\n",
        "\n",
        "            x1.append( train_validate_features[image][0] )\n",
        "\n",
        "            x2.append(x2_seq)\n",
        "\n",
        "            y.append(y_seq)\n",
        "               \n",
        "    return np.array(x1), np.array(x2), np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHMW2h4mPKpc"
      },
      "source": [
        "train_x1, train_x2, train_y = prepare_data( train_validate_images[0:7081] )\n",
        "validate_x1, validate_x2, validate_y = prepare_data( train_validate_images[7081:8081] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLEIigdwPL5o"
      },
      "source": [
        "len(train_x1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXGkapaWPNbD"
      },
      "source": [
        "len(validate_x1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTXqa5DaPRPs"
      },
      "source": [
        "#Final Model\n",
        "# feature extractor model\n",
        "input_1 = Input(shape=(4096,))\n",
        "droplayer = Dropout(0.5)(input_1)\n",
        "denselayer = Dense(256, activation='relu')(droplayer)\n",
        "\n",
        "# sequence model\n",
        "input_2 = Input(shape=(max_len,))\n",
        "embedding = Embedding(vocab_len, 256, mask_zero=True)(input_2)\n",
        "droplayer_ = Dropout(0.5)(embedding)\n",
        "lstm = LSTM(256)(droplayer_)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([denselayer, lstm])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_len, activation='softmax')(decoder2)\n",
        "\n",
        "# tie it together [image, seq] [word]\n",
        "model = Model(inputs=[input_1, input_2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# summarize model\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TAZaq9nPTxl"
      },
      "source": [
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Z9vU4ePW5v"
      },
      "source": [
        "#Reducing Overfitting\n",
        "# define checkpoint callback\n",
        "filepath = './model-ep{epoch:02d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\n",
        "callbacks = [\n",
        "             ModelCheckpoint(filepath= filepath, save_best_only=True, monitor='val_loss') ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_xCvJJxPb-K"
      },
      "source": [
        "print(\"shape of train_x1 \", train_x1.shape)\n",
        "print(\"shape of train_x2 \", train_x2.shape)\n",
        "print(\"shape of train_y \", train_y.shape)\n",
        "print()\n",
        "print(\"shape of validate_x1 \", validate_x1.shape)\n",
        "print(\"shape of validate_x2 \", validate_x2.shape)\n",
        "print(\"shape of validate_y \", validate_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i6S12AEPilU"
      },
      "source": [
        "#Training Model - Part 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RItpJnniPjk6"
      },
      "source": [
        "# fit model\n",
        "history = model.fit([train_x1, train_x2],  \n",
        "                    train_y,              \n",
        "                    verbose = 1,            \n",
        "                    epochs = 2,            \n",
        "                    callbacks = callbacks, \n",
        "                    validation_data=([validate_x1, validate_x2], validate_y)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp3c8AnZPlbs"
      },
      "source": [
        "# plot training loss and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XfolAXLPnzT"
      },
      "source": [
        "# saving the model with last parameter \n",
        "model.save('./latest_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct4kQ5kBPppC"
      },
      "source": [
        "#Evaluate the model - Part 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhDdqPIRPqhM"
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "def extract_feat(filename):\n",
        "    # load the model\n",
        "    model = VGG16()\n",
        "    # re-structure the model\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    # load the photo\n",
        "    image = load_img(filename, target_size=(224, 224))\n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for the model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # prepare the image for the VGG model\n",
        "    image = preprocess_input(image)\n",
        "    # get features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    return feature\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizr):\n",
        "    for word, index in tokenizr.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd5YLlzlPtug"
      },
      "source": [
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyrL2_KCPxVj"
      },
      "source": [
        "#Evaluating model on training images using the latest model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1SoW_nrPy5l"
      },
      "source": [
        "from keras.models import load_model\n",
        "# load the model\n",
        "modl = load_model('./latest_model.h5')\n",
        "\n",
        "# generate description\n",
        "tokenizr = Tokenizer()\n",
        "tokenizr.fit_on_texts([caption for image, caption in new_captions_dict.items() if image in train_validate_images])\n",
        "max_length = 34\n",
        "\n",
        "for count in range(10):\n",
        "\n",
        "    photo = extract_feat('{}.jpg'.format(image_dataset_path+'/'+train_validate_images[count]))  \n",
        "\n",
        "    # generate description\n",
        "    description = generate_desc(modl, tokenizr, photo, max_length)\n",
        "    print(\"Predicted caption -> \", description)\n",
        "    print()\n",
        "    print(\"Actual caption -> \", new_captions_dict[train_validate_images[count]])\n",
        "    print('*********************************************************************')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}